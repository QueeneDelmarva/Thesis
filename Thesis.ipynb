{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QueeneDelmarva/Thesis/blob/main/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Thesis Code**"
      ],
      "metadata": {
        "id": "JuXjnvCDUNzk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "vA2DqlMRUwCX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XbNhQF4XUJRc"
      },
      "outputs": [],
      "source": [
        "# Connect to Gdrive\n",
        "from google.colab import drive\n",
        "\n",
        "# Connect to Gsheets\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# Preprocessing\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Edit Distance\n",
        "import Levenshtein\n",
        "\n",
        "# GUI\n",
        "import tkinter as tk\n",
        "import tkinter.font as tkFont\n",
        "\n",
        "import string\n",
        "from google.colab import auth\n",
        "from google.auth import default"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to Gdrive"
      ],
      "metadata": {
        "id": "aoNysK_-Xwnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ONK66ZwtXwtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c825b69e-61ae-4d8e-adcc-4ee81b339b6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting to GSheets"
      ],
      "metadata": {
        "id": "AXJU0KlqU7EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the scope and create the credentials using the file in your Google Drive\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name('/content/drive/MyDrive/skripsi-394804-a98ea9715aa4.json', scope)\n",
        "\n",
        "# Authenticate with gspread\n",
        "client = gspread.authorize(creds)\n",
        "\n",
        "# Open the Google Sheet by its title or URL\n",
        "spreadsheet_key = '1RlZ8-XwwEueuCAq4nXGwq3ZmPDE-_0gP3PsroKeNFGo'\n",
        "sheet = client.open_by_key(spreadsheet_key).sheet1\n",
        "\n",
        "# Open the train_data Google Sheet by its title or URL for writing\n",
        "train_data_sheet = client.open_by_key(spreadsheet_key).worksheet('train_data')\n",
        "\n",
        "# ## Read the data from the Gsheets\n",
        "# # 1. Read individual cells\n",
        "# cell_value = sheet.cell(3, 5).value\n",
        "# print(cell_value)\n",
        "\n",
        "# # 2. Read all values from the first worksheet\n",
        "# data = sheet.get_all_values()\n",
        "\n",
        "# # Print the data\n",
        "# for row in data:\n",
        "#     print(row)"
      ],
      "metadata": {
        "id": "-OYarUCrU7Sr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mwQHmQgfd3uQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175d5c9e-61fa-4dac-b815-ef3152d7b8df"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "IaSWQ06nhAvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess_dictonary = ['Noted', 'and', 'thanks', 'Please', 'as', 'follows', 'Copied']\n",
        "\n",
        "def preprocess(word):\n",
        "    # Separate words based on non-alphabetic characters\n",
        "    words = re.findall(r'\\b(?:[a-zA-Z0-9]+)\\b', word)\n",
        "\n",
        "    # Convert words to lowercase\n",
        "    lowercase_words = [w.lower() for w in words]\n",
        "\n",
        "    return lowercase_words\n",
        "\n",
        "    # Add preprocess dictonary to the list of preprocessed words\n",
        "    # preprocessed_words = lowercase_words + [g for g in preprocess_dictonary if g in word.lower()]\n",
        "\n",
        "    # return preprocessed_words"
      ],
      "metadata": {
        "id": "6eRtztjshA7P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Radix Trie Node"
      ],
      "metadata": {
        "id": "eZjAa09wctzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a class representing a single node in the radix trie, holding information about its children and whether it marks the end of a word.\n",
        "class RadixTrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False"
      ],
      "metadata": {
        "id": "xgsecwRqcNLa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hashmap-based Trie Node"
      ],
      "metadata": {
        "id": "M1WQWZ3xWTrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a class representing a single node in the radix trie, holding information about its children and whether it marks the end of a word.\n",
        "class HashmapTrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False"
      ],
      "metadata": {
        "id": "7m3jFQTxWT0S"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Radix Trie Function"
      ],
      "metadata": {
        "id": "jvwBCoNActvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defines a class that implements a radix trie data structure, allowing efficient storage and retrieval of a set of strings with common prefixes.\n",
        "class RadixTrie:\n",
        "    def __init__(self):\n",
        "        self.root = RadixTrieNode()\n",
        "\n",
        "    # Adds a word to the Radix Trie by creating or traversing the appropriate nodes for each character in the word.\n",
        "    def insert(self, word):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = RadixTrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "\n",
        "    # Finds all words in the Radix Trie that have the given prefix, returning them as a list of results.\n",
        "    def search(self, prefix):\n",
        "        node = self.root\n",
        "        for char in prefix:\n",
        "            if char not in node.children:\n",
        "                return []  # Prefix not found, return an empty list\n",
        "            node = node.children[char]\n",
        "        return self.collect_words(node, prefix)\n",
        "\n",
        "   # Recursively explores the Radix Trie's child nodes and appends the complete words to the results list when reaching the end of a word.\n",
        "    def collect_words(self, node, current_prefix):\n",
        "        results = []\n",
        "        if node.is_end_of_word:\n",
        "            results.append(current_prefix)\n",
        "        for char, child in node.children.items():\n",
        "            word = current_prefix + char\n",
        "            results.extend(self.collect_words(child, word))\n",
        "        return results\n",
        "\n",
        "    def get_all_words(self):\n",
        "        return self.collect_words(self.root, \"\")"
      ],
      "metadata": {
        "id": "GHu5eLPQctnc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hashmap-based Trie"
      ],
      "metadata": {
        "id": "rrvnOAoDZ0vm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class represents a trie data structure that uses a hashmap to store children nodes, allowing efficient storage and retrieval of a set of strings with common prefixes.\n",
        "class HashmapTrie:\n",
        "    def __init__(self):\n",
        "        self.root = HashmapTrieNode()\n",
        "\n",
        "# Adds a word to the trie by traversing and creating nodes for each character in the word and marking the last node as the end of the word.\n",
        "    def insert(self, word):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = HashmapTrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "\n",
        "# Checks if a given prefix exists in the trie by traversing the nodes based on the characters of the prefix and returns whether the last node reached marks the end of a word.\n",
        "    def search(self, prefix):\n",
        "        node = self.root\n",
        "        for char in prefix:\n",
        "            if char not in node.children:\n",
        "                return []  # Prefix not found, return an empty list\n",
        "            node = node.children[char]\n",
        "        return self.collect_words(node, prefix)\n",
        "\n",
        "# Recursively traverses its child nodes and appends the complete words to the results list when reaching the end of a word.\n",
        "    def collect_words(self, node, current_prefix):\n",
        "        results = []\n",
        "        if node.is_end_of_word:\n",
        "            results.append(current_prefix)\n",
        "        for char, child in node.children.items():\n",
        "            word = current_prefix + char\n",
        "            results.extend(self.collect_words(child, word))\n",
        "        return results\n",
        "\n",
        "# Returns all words stored in the trie by collecting words\n",
        "    def get_all_words(self):\n",
        "        return self.collect_words(self.root, \"\")"
      ],
      "metadata": {
        "id": "MZHzghuJZ0_P"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autocorrect with Levenshtein Edit Distance"
      ],
      "metadata": {
        "id": "NgLJc-4mLSaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autocorrect:\n",
        "    def __init__(self, trie, threshold=2):\n",
        "        self.trie = trie\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def correct(self, word):\n",
        "        suggestions = []\n",
        "        self._correct_recursive(self.trie.root, \"\", word, self.threshold, suggestions)\n",
        "        if suggestions:\n",
        "            return suggestions[0]\n",
        "        return None\n",
        "\n",
        "    def _correct_recursive(self, node, prefix, target, distance_threshold, suggestions):\n",
        "      if node.is_end_of_word:\n",
        "        distance = self._calculate_distance(prefix, target)\n",
        "        if distance <= distance_threshold:\n",
        "          suggestions.append(prefix)\n",
        "\n",
        "      if not target or target[0] not in node.children:\n",
        "        return\n",
        "\n",
        "      child = node.children[target[0]]\n",
        "      self._correct_recursive(child, prefix + target[0], target[1:], distance_threshold, suggestions)\n",
        "      self._correct_recursive(child, prefix, target[1:], distance_threshold, suggestions)\n",
        "      self._correct_recursive(child, prefix + target[0] + target[1:], target[2:], distance_threshold, suggestions)\n",
        "\n",
        "    def _calculate_distance(self, source, target):\n",
        "        m, n = len(source), len(target)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(m + 1):\n",
        "            dp[i][0] = i\n",
        "\n",
        "        for j in range(n + 1):\n",
        "            dp[0][j] = j\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if source[i - 1] == target[j - 1]:\n",
        "                    cost = 0\n",
        "                else:\n",
        "                    cost = 1\n",
        "                dp[i][j] = min(dp[i - 1][j] + 1, dp[i][j - 1] + 1, dp[i - 1][j - 1] + cost)\n",
        "\n",
        "        return dp[m][n]\n"
      ],
      "metadata": {
        "id": "cqCB_rlfLSRB"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Main Function**"
      ],
      "metadata": {
        "id": "z0GsjWoqVe29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Read all values from the original_data worksheet\n",
        "    data = sheet.get_all_values()\n",
        "\n",
        "\n",
        "    # Create instances of the trie structures and the autocorrect class\n",
        "    radix_trie = RadixTrie()\n",
        "    hashmap_trie = HashmapTrie()\n",
        "    radix_autocorrect = Autocorrect(radix_trie, threshold=3)\n",
        "    hashmap_autocorrect = Autocorrect(hashmap_trie, threshold=3)\n",
        "\n",
        "    # Collect preprocessed words for batch writes\n",
        "    radix_batch_data = []\n",
        "    hashmap_batch_data = []\n",
        "\n",
        "    # Take user input\n",
        "    user_input = input(\"Enter a word: \")\n",
        "\n",
        "    # Autocorrect the user input\n",
        "    corrected_word_radix = radix_autocorrect.correct(user_input)\n",
        "    corrected_word_hashmap = hashmap_autocorrect.correct(user_input)\n",
        "\n",
        "    # Display autocorrected results\n",
        "    print(f\"Radix Autocorrect: {corrected_word_radix}\")\n",
        "    print(f\"Hashmap Autocorrect: {corrected_word_hashmap}\")\n",
        "\n",
        "\n",
        "    # # Loop\n",
        "    # for row in data:\n",
        "    #     for word in row:\n",
        "    #         preprocessed_words = preprocess(word)\n",
        "    #         for preprocessed_word in preprocessed_words:\n",
        "    #             if preprocessed_word:  # Skip empty words after preprocessing\n",
        "    #                 radix_trie.insert(preprocessed_word)\n",
        "    #                 hashmap_trie.insert(preprocessed_word)\n",
        "\n",
        "    #                 hashmap_trie.insert(preprocessed_word)\n",
        "    #                 hashmap_batch_data.append([preprocessed_word])\n",
        "\n",
        "    # # Use batch writes to save preprocessed words in one API call for each trie structure\n",
        "    # # train_data_sheet.append_rows(radix_batch_data)\n",
        "    # # train_data_sheet.append_rows(hashmap_batch_data)\n",
        "\n",
        "    #                 # Use the Autocorrect instances\n",
        "    #                 corrected_word_radix = radix_autocorrect.correct(preprocessed_word)\n",
        "    #                 corrected_word_hashmap = hashmap_autocorrect.correct(preprocessed_word)\n",
        "\n",
        "    #                 if corrected_word_radix:\n",
        "    #                     train_data_sheet.append_row([f\"Radix Autocorrect: {corrected_word_radix}\"])\n",
        "\n",
        "    #                 if corrected_word_hashmap:\n",
        "    #                     train_data_sheet.append_row([f\"Hashmap Autocorrect: {corrected_word_hashmap}\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "5VoBr7nSVert",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97ce695-2f2a-43db-c19d-0e3c6288bb60"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: Stoop\n",
            "Radix Autocorrect: None\n",
            "Hashmap Autocorrect: None\n"
          ]
        }
      ]
    }
  ]
}