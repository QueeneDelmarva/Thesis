{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QueeneDelmarva/Thesis/blob/main/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuXjnvCDUNzk"
      },
      "source": [
        "# **Thesis Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA2DqlMRUwCX"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XbNhQF4XUJRc"
      },
      "outputs": [],
      "source": [
        "# Connect to Gdrive\n",
        "from google.colab import drive\n",
        "\n",
        "# Connect to Gsheets\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# Preprocessing\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Edit Distance\n",
        "import Levenshtein\n",
        "\n",
        "# GUI\n",
        "import tkinter as tk\n",
        "import tkinter.font as tkFont\n",
        "\n",
        "import string\n",
        "from google.colab import auth\n",
        "from google.auth import default"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-Levenshtein\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBQoMsLMwiOR",
        "outputId": "e2147fcd-4b9f-431b-a292-19c233f3de5d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.21.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting Levenshtein==0.21.1 (from python-Levenshtein)\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein==0.21.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 python-Levenshtein-0.21.1 rapidfuzz-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNysK_-Xwnx"
      },
      "source": [
        "## Connecting to Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ONK66ZwtXwtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa50fae9-1381-4737-ceb9-b461843810ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXJU0KlqU7EF"
      },
      "source": [
        "## Connecting to GSheets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-OYarUCrU7Sr"
      },
      "outputs": [],
      "source": [
        "# Define the scope and create the credentials using the file in your Google Drive\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name('/content/drive/MyDrive/skripsi-394804-a98ea9715aa4.json', scope)\n",
        "\n",
        "# Authenticate with gspread\n",
        "client = gspread.authorize(creds)\n",
        "\n",
        "# Open the Google Sheet by its title or URL\n",
        "spreadsheet_key = '1RlZ8-XwwEueuCAq4nXGwq3ZmPDE-_0gP3PsroKeNFGo'\n",
        "sheet = client.open_by_key(spreadsheet_key).sheet1\n",
        "\n",
        "# Open the train_data Google Sheet by its title or URL for writing\n",
        "train_data_sheet = client.open_by_key(spreadsheet_key).worksheet('train_data')\n",
        "\n",
        "# ## Read the data from the Gsheets\n",
        "# # 1. Read individual cells\n",
        "# cell_value = sheet.cell(3, 5).value\n",
        "# print(cell_value)\n",
        "\n",
        "# # 2. Read all values from the first worksheet\n",
        "# data = sheet.get_all_values()\n",
        "\n",
        "# # Print the data\n",
        "# for row in data:\n",
        "#     print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaSWQ06nhAvH"
      },
      "source": [
        "## Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6eRtztjshA7P"
      },
      "outputs": [],
      "source": [
        "# preprocess_dictonary = ['Noted', 'and', 'thanks', 'Please', 'as', 'follows', 'Copied']\n",
        "\n",
        "def preprocess(word):\n",
        "    # Separate words based on non-alphabetic characters\n",
        "    words = re.findall(r'\\b(?:[a-zA-Z0-9]+)\\b', word)\n",
        "\n",
        "    # Convert words to lowercase\n",
        "    lowercase_words = [w.lower() for w in words]\n",
        "\n",
        "    return lowercase_words\n",
        "\n",
        "    # Add preprocess dictonary to the list of preprocessed words\n",
        "    # preprocessed_words = lowercase_words + [g for g in preprocess_dictonary if g in word.lower()]\n",
        "\n",
        "    # return preprocessed_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZjAa09wctzp"
      },
      "source": [
        "### Radix Trie Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xgsecwRqcNLa"
      },
      "outputs": [],
      "source": [
        "# Defines a class representing a single node in the radix trie, holding information about its children and whether it marks the end of a word.\n",
        "class RadixTrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1WQWZ3xWTrv"
      },
      "source": [
        "### Hashmap-based Trie Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7m3jFQTxWT0S"
      },
      "outputs": [],
      "source": [
        "# Defines a class representing a single node in the radix trie, holding information about its children and whether it marks the end of a word.\n",
        "class HashmapTrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvwBCoNActvv"
      },
      "source": [
        "### Radix Trie Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GHu5eLPQctnc"
      },
      "outputs": [],
      "source": [
        "# Defines a class that implements a radix trie data structure, allowing efficient storage and retrieval of a set of strings with common prefixes.\n",
        "class RadixTrie:\n",
        "    def __init__(self):\n",
        "        self.root = RadixTrieNode()\n",
        "\n",
        "    # Adds a word to the Radix Trie by creating or traversing the appropriate nodes for each character in the word.\n",
        "    def insert(self, word):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = RadixTrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "\n",
        "    # Finds all words in the Radix Trie that have the given prefix, returning them as a list of results.\n",
        "    def search(self, prefix):\n",
        "        node = self.root\n",
        "        for char in prefix:\n",
        "            if char not in node.children:\n",
        "                return []  # Prefix not found, return an empty list\n",
        "            node = node.children[char]\n",
        "        return self.collect_words(node, prefix)\n",
        "\n",
        "   # Recursively explores the Radix Trie's child nodes and appends the complete words to the results list when reaching the end of a word.\n",
        "    def collect_words(self, node, current_prefix):\n",
        "        results = []\n",
        "        if node.is_end_of_word:\n",
        "            results.append(current_prefix)\n",
        "        for char, child in node.children.items():\n",
        "            word = current_prefix + char\n",
        "            results.extend(self.collect_words(child, word))\n",
        "        return results\n",
        "\n",
        "    def get_all_words(self):\n",
        "        return self.collect_words(self.root, \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrvnOAoDZ0vm"
      },
      "source": [
        "### Hashmap-based Trie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MZHzghuJZ0_P"
      },
      "outputs": [],
      "source": [
        "# Define a class represents a trie data structure that uses a hashmap to store children nodes, allowing efficient storage and retrieval of a set of strings with common prefixes.\n",
        "class HashmapTrie:\n",
        "    def __init__(self):\n",
        "        self.root = HashmapTrieNode()\n",
        "\n",
        "# Adds a word to the trie by traversing and creating nodes for each character in the word and marking the last node as the end of the word.\n",
        "    def insert(self, word):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = HashmapTrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "\n",
        "# Checks if a given prefix exists in the trie by traversing the nodes based on the characters of the prefix and returns whether the last node reached marks the end of a word.\n",
        "    def search(self, prefix):\n",
        "        node = self.root\n",
        "        for char in prefix:\n",
        "            if char not in node.children:\n",
        "                return []  # Prefix not found, return an empty list\n",
        "            node = node.children[char]\n",
        "        return self.collect_words(node, prefix)\n",
        "\n",
        "# Recursively traverses its child nodes and appends the complete words to the results list when reaching the end of a word.\n",
        "    def collect_words(self, node, current_prefix):\n",
        "        results = []\n",
        "        if node.is_end_of_word:\n",
        "            results.append(current_prefix)\n",
        "        for char, child in node.children.items():\n",
        "            word = current_prefix + char\n",
        "            results.extend(self.collect_words(child, word))\n",
        "        return results\n",
        "\n",
        "# Returns all words stored in the trie by collecting words\n",
        "    def get_all_words(self):\n",
        "        return self.collect_words(self.root, \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgLJc-4mLSaM"
      },
      "source": [
        "### Autocorrect with Levenshtein Edit Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cqCB_rlfLSRB"
      },
      "outputs": [],
      "source": [
        "class Autocorrect:\n",
        "    def __init__(self, trie):\n",
        "        self.trie = trie\n",
        "\n",
        "    def correct(self, target, distance_threshold=2):\n",
        "        suggestions = []\n",
        "        for char in self.trie.root.children:\n",
        "            self._correct_recursive(self.trie.root.children[char], char, target, distance_threshold, suggestions)\n",
        "        return suggestions\n",
        "\n",
        "    def _correct_recursive(self, node, prefix, target, distance_threshold, suggestions):\n",
        "        if node.is_end_of_word:\n",
        "            distance = self._calculate_distance(prefix, target)\n",
        "            if distance <= distance_threshold:\n",
        "                suggestions.append(prefix)\n",
        "\n",
        "        if not target or target[0] not in node.children:\n",
        "            return\n",
        "\n",
        "        child = node.children[target[0]]\n",
        "        self._correct_recursive(child, prefix + target[0], target[1:], distance_threshold, suggestions)\n",
        "        self._correct_recursive(child, prefix, target[1:], distance_threshold, suggestions)\n",
        "        self._correct_recursive(child, prefix + target[0] + target[1:], target[2:], distance_threshold, suggestions)\n",
        "\n",
        "    def _calculate_distance(self, str1, str2):\n",
        "        return Levenshtein.distance(str1, str2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0GsjWoqVe29"
      },
      "source": [
        "### **Main Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5VoBr7nSVert",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "df2277a0-3229-4fde-eb23-17b67508e3e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word (or type 'exit' to quit): Dtae\n",
            "Closest Match: eta\n",
            "Enter a word (or type 'exit' to quit): Date\n",
            "Closest Match: date\n",
            "Enter a word (or type 'exit' to quit): Nited\n",
            "Closest Match: noted\n",
            "Enter a word (or type 'exit' to quit): Goo\n",
            "Closest Match: to\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3cb65dc2c383>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-3cb65dc2c383>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# User input for autocorrection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter a word (or type 'exit' to quit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    # Read all values from the original_data worksheet\n",
        "    data = sheet.get_all_values()\n",
        "\n",
        "    # Create instances of the trie structures and the autocorrect class\n",
        "    radix_trie = RadixTrie()\n",
        "    hashmap_trie = HashmapTrie()\n",
        "    radix_autocorrect = Autocorrect(radix_trie)\n",
        "    hashmap_autocorrect = Autocorrect(hashmap_trie)\n",
        "\n",
        "    # Collect preprocessed words for batch writes\n",
        "    radix_batch_data = []\n",
        "    hashmap_batch_data = []\n",
        "\n",
        "    # Preprocess\n",
        "    for row in data:\n",
        "        for word in row:\n",
        "            preprocessed_words = preprocess(word)\n",
        "            for preprocessed_word in preprocessed_words:\n",
        "                if preprocessed_word:  # Skip empty words after preprocessing\n",
        "                    radix_trie.insert(preprocessed_word)\n",
        "                    hashmap_trie.insert(preprocessed_word)\n",
        "\n",
        "                    hashmap_trie.insert(preprocessed_word)\n",
        "                    hashmap_batch_data.append([preprocessed_word])\n",
        "\n",
        "                    # Use batch writes to save preprocessed words in one API call for each trie structure\n",
        "                    # train_data_sheet.append_rows(radix_batch_data)\n",
        "                    # train_data_sheet.append_rows(hashmap_batch_data)\n",
        "\n",
        "    # User input for autocorrection\n",
        "    while True:\n",
        "        user_input = input(\"Enter a word (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        # Fetch correct words from the train_data sheet\n",
        "        correct_words = train_data_sheet.col_values(1)  # Assuming correct words are in the first column\n",
        "\n",
        "        # Autocorrect the user input using Levenshtein distance\n",
        "        closest_match = None\n",
        "        closest_distance = float('inf')\n",
        "        for correct_word in correct_words:\n",
        "            distance = Levenshtein.distance(user_input, correct_word)\n",
        "            if distance < closest_distance:\n",
        "                closest_distance = distance\n",
        "                closest_match = correct_word\n",
        "\n",
        "        # Display the closest match and append it to the train_data sheet\n",
        "        print(f\"Closest Match: {closest_match}\")\n",
        "        if closest_match != user_input:\n",
        "            train_data_sheet.append_row([f\"User Input: {user_input}, Closest Match: {closest_match}\"])\n",
        "\n",
        "\n",
        "                    # Use the Autocorrect instances\n",
        "                    # corrected_word_radix = radix_autocorrect.correct(preprocessed_word)\n",
        "                    # corrected_word_hashmap = hashmap_autocorrect.correct(preprocessed_word)\n",
        "\n",
        "                    # if corrected_word_radix:\n",
        "                    #     train_data_sheet.append_row([f\"Radix Autocorrect: {corrected_word_radix}\"])\n",
        "\n",
        "                    # if corrected_word_hashmap:\n",
        "                    #     train_data_sheet.append_row([f\"Hashmap Autocorrect: {corrected_word_hashmap}\"])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}