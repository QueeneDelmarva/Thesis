{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/QueeneDelmarva/Thesis/blob/main/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuXjnvCDUNzk"
      },
      "source": [
        "# **Thesis Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA2DqlMRUwCX"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "XbNhQF4XUJRc",
        "outputId": "9ee21df6-4212-4ce5-e867-169a4becf050"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-d17a707d22d2>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Edit Distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLevenshtein\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0myour_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLongestCommonSubsequence\u001b[0m  \u001b[0;31m# Import the LongestCommonSubsequence function from your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Generate Misspellings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'your_module'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# Connect to Gdrive\n",
        "from google.colab import drive\n",
        "\n",
        "# Connect to Gsheets\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# Preprocessing\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Edit Distance\n",
        "import Levenshtein\n",
        "import difflib\n",
        "\n",
        "# Generate Misspellings\n",
        "import random\n",
        "import string\n",
        "\n",
        "# GUI\n",
        "import tkinter as tk\n",
        "import tkinter.font as tkFont\n",
        "\n",
        "import string\n",
        "from google.colab import auth\n",
        "from google.auth import default"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install python-Levenshtein\n"
      ],
      "metadata": {
        "id": "jBQoMsLMwiOR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoNysK_-Xwnx"
      },
      "source": [
        "## Connecting to Gdrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ONK66ZwtXwtD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d22ac061-24d4-477d-b463-03f66a4437d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXJU0KlqU7EF"
      },
      "source": [
        "## Connecting to GSheets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-OYarUCrU7Sr"
      },
      "outputs": [],
      "source": [
        "# Define the scope and create the credentials using the file in your Google Drive\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "creds = ServiceAccountCredentials.from_json_keyfile_name('/content/drive/MyDrive/skripsi-394804-a98ea9715aa4.json', scope)\n",
        "\n",
        "# Authenticate with gspread\n",
        "client = gspread.authorize(creds)\n",
        "\n",
        "# Open the Google Sheet by its title or URL\n",
        "spreadsheet_key = '1RlZ8-XwwEueuCAq4nXGwq3ZmPDE-_0gP3PsroKeNFGo'\n",
        "sheet = client.open_by_key(spreadsheet_key).sheet1\n",
        "\n",
        "# Open the train_data Google Sheet by its title or URL for writing\n",
        "train_data_sheet = client.open_by_key(spreadsheet_key).worksheet('train_data')\n",
        "test_data_sheet = client.open_by_key(spreadsheet_key).worksheet('test_data')\n",
        "\n",
        "# ## Read the data from the Gsheets\n",
        "# # 1. Read individual cells\n",
        "# cell_value = sheet.cell(3, 5).value\n",
        "# print(cell_value)\n",
        "\n",
        "# # 2. Read all values from the first worksheet\n",
        "# data = sheet.get_all_values()\n",
        "\n",
        "# # Print the data\n",
        "# for row in data:\n",
        "#     print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaSWQ06nhAvH"
      },
      "source": [
        "## Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6eRtztjshA7P"
      },
      "outputs": [],
      "source": [
        "# preprocess_dictonary = ['Noted', 'and', 'thanks', 'Please', 'as', 'follows', 'Copied']\n",
        "\n",
        "def preprocess(word):\n",
        "    # Separate words based on non-alphabetic characters\n",
        "    words = re.findall(r'\\b(?:[a-zA-Z0-9]+)\\b', word)\n",
        "\n",
        "    # Convert words to lowercase\n",
        "    lowercase_words = [w.lower() for w in words]\n",
        "\n",
        "    return lowercase_words\n",
        "\n",
        "    # Add preprocess dictonary to the list of preprocessed words\n",
        "    # preprocessed_words = lowercase_words + [g for g in preprocess_dictonary if g in word.lower()]\n",
        "\n",
        "    # return preprocessed_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZjAa09wctzp"
      },
      "source": [
        "### Radix Trie Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xgsecwRqcNLa"
      },
      "outputs": [],
      "source": [
        "# Defines a class representing a single node in the radix trie, holding information about its children and whether it marks the end of a word.\n",
        "class RadixTrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1WQWZ3xWTrv"
      },
      "source": [
        "### Hashmap-based Trie Node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7m3jFQTxWT0S"
      },
      "outputs": [],
      "source": [
        "# Defines a class representing a single node in the radix trie, holding information about its children and whether it marks the end of a word.\n",
        "class HashmapTrieNode:\n",
        "    def __init__(self):\n",
        "        self.children = {}\n",
        "        self.is_end_of_word = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvwBCoNActvv"
      },
      "source": [
        "### Radix Trie Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GHu5eLPQctnc"
      },
      "outputs": [],
      "source": [
        "# Defines a class that implements a radix trie data structure, allowing efficient storage and retrieval of a set of strings with common prefixes.\n",
        "class RadixTrie:\n",
        "    def __init__(self):\n",
        "        self.root = RadixTrieNode()\n",
        "\n",
        "    # Adds a word to the Radix Trie by creating or traversing the appropriate nodes for each character in the word.\n",
        "    def insert(self, word):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = RadixTrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "\n",
        "    # Finds all words in the Radix Trie that have the given prefix, returning them as a list of results.\n",
        "    def search(self, prefix):\n",
        "        node = self.root\n",
        "        for char in prefix:\n",
        "            if char not in node.children:\n",
        "                return []  # Prefix not found, return an empty list\n",
        "            node = node.children[char]\n",
        "        return self.collect_words(node, prefix)\n",
        "\n",
        "   # Recursively explores the Radix Trie's child nodes and appends the complete words to the results list when reaching the end of a word.\n",
        "    def collect_words(self, node, current_prefix):\n",
        "        results = []\n",
        "        if node.is_end_of_word:\n",
        "            results.append(current_prefix)\n",
        "        for char, child in node.children.items():\n",
        "            word = current_prefix + char\n",
        "            results.extend(self.collect_words(child, word))\n",
        "        return results\n",
        "\n",
        "    def get_all_words(self):\n",
        "        return self.collect_words(self.root, \"\")\n",
        "\n",
        "    # def get_all_terms(self):\n",
        "    #     # Collect all terms stored in the trie\n",
        "    #     all_terms = []\n",
        "\n",
        "    #     def traverse(node, prefix):\n",
        "    #         if node.is_end_of_word:\n",
        "    #             all_terms.append(prefix)\n",
        "    #         for char, child in node.children.items():\n",
        "    #             traverse(child, prefix + char)\n",
        "\n",
        "    #     traverse(self.root, \"\")\n",
        "\n",
        "    #     return all_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrvnOAoDZ0vm"
      },
      "source": [
        "### Hashmap-based Trie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MZHzghuJZ0_P"
      },
      "outputs": [],
      "source": [
        "# Define a class represents a trie data structure that uses a hashmap to store children nodes, allowing efficient storage and retrieval of a set of strings with common prefixes.\n",
        "class HashmapTrie:\n",
        "    def __init__(self):\n",
        "        self.root = HashmapTrieNode()\n",
        "\n",
        "# Adds a word to the trie by traversing and creating nodes for each character in the word and marking the last node as the end of the word.\n",
        "    def insert(self, word):\n",
        "        node = self.root\n",
        "        for char in word:\n",
        "            if char not in node.children:\n",
        "                node.children[char] = HashmapTrieNode()\n",
        "            node = node.children[char]\n",
        "        node.is_end_of_word = True\n",
        "\n",
        "# Checks if a given prefix exists in the trie by traversing the nodes based on the characters of the prefix and returns whether the last node reached marks the end of a word.\n",
        "    def search(self, prefix):\n",
        "        node = self.root\n",
        "        for char in prefix:\n",
        "            if char not in node.children:\n",
        "                return []  # Prefix not found, return an empty list\n",
        "            node = node.children[char]\n",
        "        return self.collect_words(node, prefix)\n",
        "\n",
        "# Recursively traverses its child nodes and appends the complete words to the results list when reaching the end of a word.\n",
        "    def collect_words(self, node, current_prefix):\n",
        "        results = []\n",
        "        if node.is_end_of_word:\n",
        "            results.append(current_prefix)\n",
        "        for char, child in node.children.items():\n",
        "            word = current_prefix + char\n",
        "            results.extend(self.collect_words(child, word))\n",
        "        return results\n",
        "\n",
        "# Returns all words stored in the trie by collecting words\n",
        "    def get_all_words(self):\n",
        "        return self.collect_words(self.root, \"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgLJc-4mLSaM"
      },
      "source": [
        "### Autocorrect with Levenshtein Edit Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cqCB_rlfLSRB"
      },
      "outputs": [],
      "source": [
        "class Autocorrect:\n",
        "    def __init__(self, trie):\n",
        "        self.trie = trie\n",
        "\n",
        "    def correct(self, word, threshold=0.9):  # Adjust threshold as needed\n",
        "        word = word.lower()\n",
        "        suggestions = []\n",
        "\n",
        "        # Traverse the trie to find similar words\n",
        "        similar_words = self.trie.search_similar(word)\n",
        "\n",
        "        # Calculate Jaro-Winkler distance and filter based on threshold\n",
        "        for similar_word in similar_words:\n",
        "            similarity = distance.get_jaro_distance(word, similar_word)\n",
        "            if similarity >= threshold:\n",
        "                suggestions.append(similar_word)\n",
        "\n",
        "        if suggestions:\n",
        "            return suggestions[0]  # Return the first suggestion\n",
        "        else:\n",
        "            return None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Misspellings"
      ],
      "metadata": {
        "id": "J7j8wSUq1H0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MisspelledWordGenerator:\n",
        "    def __init__(self, threshold=0.2):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def generate(self, word):\n",
        "        misspelled_word = []\n",
        "        for char in word:\n",
        "            if random.random() < self.threshold:\n",
        "                misspelled_word.append(random.choice(string.ascii_lowercase))\n",
        "            else:\n",
        "                misspelled_word.append(char)\n",
        "        return ''.join(misspelled_word)"
      ],
      "metadata": {
        "id": "TzVtNDue1H8C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight Parameters"
      ],
      "metadata": {
        "id": "c53ZOdVlUxun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightEvaluator:\n",
        "    def __init__(self, trie_structure):\n",
        "        self.trie_structure = trie_structure\n",
        "\n",
        "    def evaluate_weight_combinations(self, test_data, prefix_weights, suffix_weights, similarity_weights):\n",
        "        best_score = float('-inf')\n",
        "        best_weights = (0, 0, 0)  # Initialize with default weights\n",
        "        results = []\n",
        "\n",
        "        for w1 in prefix_weights:\n",
        "            for w2 in suffix_weights:\n",
        "                for w3 in similarity_weights:\n",
        "                    evaluator = EvaluationPerformance(self.trie_structure, (w1, w2, w3))\n",
        "                    evaluation_results = evaluator.evaluate(test_data)\n",
        "                    total_score = sum(overall_score for _, _, overall_score in evaluation_results)\n",
        "\n",
        "                    if total_score > best_score:\n",
        "                        best_score = total_score\n",
        "                        best_weights = (w1, w2, w3)\n",
        "                        results = evaluation_results\n",
        "\n",
        "        return best_weights, results"
      ],
      "metadata": {
        "id": "iTYarXKhUx2p"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Evaluation"
      ],
      "metadata": {
        "id": "UYLZH7Mgn6WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EvaluationPerformance:\n",
        "    def __init__(self, trie_structure, weight_settings):\n",
        "        self.trie_structure = trie_structure\n",
        "        self.w1, self.w2, self.w3 = weight_settings\n",
        "\n",
        "    def calculate_prefix_score(self, word, dictionary):\n",
        "        # Calculate and return the prefix score for the word and dictionary\n",
        "        prefix_score = 0\n",
        "        for term in dictionary:\n",
        "            if term.startswith(word):\n",
        "                prefix_score += len(word)\n",
        "        return prefix_score\n",
        "\n",
        "    def calculate_suffix_score(self, word, dictionary):\n",
        "        # Calculate and return the suffix score for the word and dictionary\n",
        "        suffix_score = 0\n",
        "        for term in dictionary:\n",
        "            reversed_term = term[::-1]\n",
        "            reversed_word = word[::-1]\n",
        "            if reversed_term.startswith(reversed_word):\n",
        "                suffix_score += len(reversed_word)\n",
        "        return suffix_score\n",
        "\n",
        "    def calculate_similarity_score(self, word, term):\n",
        "        # Calculate and return the similarity score for the word and term\n",
        "        lcs = self.LongestCommonSubsequence(word, term)\n",
        "        similarity_score = (2 * lcs) / (len(word) + len(term))\n",
        "        return similarity_score\n",
        "\n",
        "    def LongestCommonSubsequence(self, text1, text2):\n",
        "        m, n = len(text1), len(text2)\n",
        "        dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "        for i in range(1, m + 1):\n",
        "            for j in range(1, n + 1):\n",
        "                if text1[i - 1] == text2[j - 1]:\n",
        "                    dp[i][j] = dp[i - 1][j - 1] + 1\n",
        "                else:\n",
        "                    dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])\n",
        "\n",
        "        return dp[m][n]\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        results = []\n",
        "        for query, correct_term in test_data:\n",
        "            dictionary = self.trie_structure.get_all_words()  # Use get_all_words() to get dictionary terms\n",
        "            prefix_score = self.calculate_prefix_score(query, dictionary)\n",
        "            suffix_score = self.calculate_suffix_score(query, dictionary)\n",
        "            similarity_score = self.calculate_similarity_score(query, dictionary)\n",
        "\n",
        "            # Calculate the overall score based on weights w1, w2, and w3\n",
        "            overall_score = (self.w1 * prefix_score) + (self.w2 * suffix_score) + (self.w3 * similarity_score)\n",
        "\n",
        "            results.append((query, correct_term, overall_score))\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "cnfO-rB9n6j1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2\n",
        "class EvaluationPerformance:\n",
        "    def __init__(self, weight_settings):\n",
        "        self.w1, self.w2, self.w3 = weight_settings\n",
        "        self.radix_trie = RadixTrie()\n",
        "        self.hashmap_trie = HashmapTrie()\n",
        "\n",
        "    def calculate_prefix_score(self, word, dictionary):\n",
        "        # Calculate and return the prefix score for the word and dictionary\n",
        "        prefix_score = 0\n",
        "        for term in dictionary:\n",
        "            if term.startswith(word):\n",
        "                prefix_score += len(word)\n",
        "        return prefix_score\n",
        "\n",
        "    def calculate_suffix_score(self, word, dictionary):\n",
        "        # Calculate and return the suffix score for the word and dictionary\n",
        "        suffix_score = 0\n",
        "        for term in dictionary:\n",
        "            reversed_term = term[::-1]\n",
        "            reversed_word = word[::-1]\n",
        "            if reversed_term.startswith(reversed_word):\n",
        "                suffix_score += len(reversed_word)\n",
        "        return suffix_score\n",
        "\n",
        "    def calculate_similarity_score(self, word, term):\n",
        "        # Calculate and return the similarity score for the word and term\n",
        "        lcs = len(LongestCommonSubsequence(word, term))\n",
        "        similarity_score = (2 * lcs) / (len(word) + len(term))\n",
        "        return similarity_score\n",
        "\n",
        "    def evaluate(self, test_data, weight_settings):\n",
        "        results = []\n",
        "        for query, correct_term in test_data:\n",
        "            prefix_score = self.calculate_prefix_score(query, self.trie_structure.get_all_words())\n",
        "            suffix_score = self.calculate_suffix_score(query, self.trie_structure.get_all_words())\n",
        "            similarity_score = self.calculate_similarity_score(query, correct_term)\n",
        "\n",
        "            w1, w2, w3 = weight_settings\n",
        "\n",
        "            # Calculate the overall score based on weights w1, w2, and w3\n",
        "            overall_score = (w1 * prefix_score) + (w2 * suffix_score) + (w3 * similarity_score)\n",
        "\n",
        "            results.append((query, correct_term, overall_score))\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "btZB45Ewia24"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Define your test data here\n",
        "    test_data = [\n",
        "        (\"groand\", \"ground\"),\n",
        "        (\"dayt\", \"date\"),\n",
        "        (\"tomorow\", \"tomorrow\"),\n",
        "        (\"request\", \"requtst\"),\n",
        "        (\"cnnficmeq\", \"confirmed\"),\n",
        "        # ... add more test cases\n",
        "    ]\n",
        "\n",
        "    # Create instances of the trie structures and the autocorrect class\n",
        "    radix_trie = RadixTrie()\n",
        "    hashmap_trie = HashmapTrie()\n",
        "\n",
        "    # Insert words into trie structures here if needed\n",
        "\n",
        "    # Evaluation settings\n",
        "    prefix_weights = [0.15, 0.20, 0.25, 0.30, 0.35]\n",
        "    suffix_weights = [0.15, 0.20, 0.25, 0.30, 0.35]\n",
        "    similarity_weights = [1.0, 1.1, 1.2]  # Include variations for similarity weight\n",
        "\n",
        "    print(\"Weight Study Results:\")\n",
        "    print(\"{:<10} | {:<10} | {:<10} | {:<10} | {:<10}\".format(\"w1\", \"w2\", \"w3\", \"Accuracy\", \"MMR\"))\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for w1 in prefix_weights:\n",
        "      for w2 in suffix_weights:\n",
        "        for w3 in similarity_weights:\n",
        "            weight_settings = (w1, w2, w3)  # Create a tuple with the weight settings\n",
        "            evaluator = EvaluationPerformance(radix_trie, hashmap_trie, weight_settings)  # Pass trie structures and weight settings\n",
        "            evaluation_results = evaluator.evaluate(test_data)\n",
        "\n",
        "            accuracy_values = [result[2] for result in evaluation_results]  # Use index 2 for Accuracy\n",
        "            mmr_values = [result[3] for result in evaluation_results]  # Use index 3 for MMR\n",
        "\n",
        "            average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
        "            average_mmr = sum(mmr_values) / len(mmr_values)\n",
        "\n",
        "            print(\"{:<10} | {:<10} | {:<10} | {:<10.3f} | {:<10.3f}\".format(w1, w2, w3, average_accuracy, average_mmr))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "o8_ccCEthz52",
        "outputId": "6032875d-0565-45a2-a3a6-fb54cea02b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Study Results:\n",
            "w1         | w2         | w3         | Accuracy   | MMR       \n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-eab1ffdb6487>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-82-eab1ffdb6487>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mw3\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimilarity_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mweight_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Create a tuple with the weight settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluationPerformance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mradix_trie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashmap_trie\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_settings\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass trie structures and weight settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mevaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: EvaluationPerformance.__init__() takes 2 positional arguments but 4 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def main():\n",
        "#     # Create instances of the trie structures and the autocorrect class\n",
        "#     radix_trie = RadixTrie()\n",
        "#     hashmap_trie = HashmapTrie()\n",
        "\n",
        "#     # Evaluation settings (prefix_weight, suffix_weight, similarity_weight)\n",
        "#     weight_settings_list = [\n",
        "#         (0.15, 0.20, 1.1),\n",
        "#         (0.20, 0.20, 1.1),\n",
        "#         (0.25, 0.20, 1.1),\n",
        "#         # ... add more weight settings\n",
        "#     ]\n",
        "\n",
        "#     print(\"Weight Study Results:\")\n",
        "#     print(\"{:<10} | {:<10} | {:<10}\".format(\"w1\", \"Accuracy\", \"MMR\"))\n",
        "#     print(\"-\" * 30)\n",
        "\n",
        "#     test_data = [\n",
        "#         (\"groand\", \"ground\"),\n",
        "#         (\"dayt\", \"date\"),\n",
        "#         (\"tomorow\", \"tomorrow\"),\n",
        "#         # ... add more test data\n",
        "#     ]\n",
        "\n",
        "#     for weight_settings in weight_settings_list:\n",
        "#         evaluator = EvaluationPerformance(radix_trie, weight_settings)\n",
        "#         evaluation_results = evaluator.evaluate(test_data)\n",
        "\n",
        "#         accuracy_values = [result[2] for result in evaluation_results]\n",
        "#         mmr_values = [result[2] for result in evaluation_results]  # Use index 2 for MMR\n",
        "\n",
        "#         average_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
        "#         average_mmr = sum(mmr_values) / len(mmr_values)\n",
        "\n",
        "#         print(\"{:<10} | {:<10.3f} | {:<10.3f}\".format(weight_settings[0], average_accuracy, average_mmr))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8xnb6McdDJD",
        "outputId": "bfe13c84-8478-48ee-b80c-6ce161eb07b6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weight Study Results:\n",
            "w1         | Accuracy   | MMR       \n",
            "------------------------------\n",
            "0.15       | 0.000      | 0.000     \n",
            "0.2        | 0.000      | 0.000     \n",
            "0.25       | 0.000      | 0.000     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0GsjWoqVe29"
      },
      "source": [
        "### **Main Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "5VoBr7nSVert",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1031c7e3-7dc3-4887-e22d-dc2c6a855c41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: PgeprjcessedgData\n",
            "Correct Term: ProcessedData\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: date\n",
            "Correct Term: date\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: from\n",
            "Correct Term: from\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: to\n",
            "Correct Term: to\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: oysjeet\n",
            "Correct Term: outset\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: contknt\n",
            "Correct Term: contact\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: 0w\n",
            "Correct Term: ow\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: 02\n",
            "Correct Term: 02\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: 2023\n",
            "Correct Term: 2023\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: raymznd\n",
            "Correct Term: raymond\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: koh\n",
            "Correct Term: koh\n",
            "Overall Score: 0.0\n",
            "---\n",
            "Query: acrb\n",
            "Correct Term: carb\n",
            "Overall Score: 0.0\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "# def main():\n",
        "#     # Read words from the train_data worksheet\n",
        "#     train_words = train_data_sheet.col_values(1)\n",
        "\n",
        "#     # Create instances of the trie structures and the autocorrect class\n",
        "#     radix_trie = RadixTrie()\n",
        "#     hashmap_trie = HashmapTrie()\n",
        "#     radix_autocorrect = Autocorrect(radix_trie)\n",
        "#     hashmap_autocorrect = Autocorrect(hashmap_trie)\n",
        "\n",
        "#     # Evaluation settings (prefix_weight, suffix_weight, similarity_weight)\n",
        "#     weight_settings = (0.25, 0.20, 1.1)\n",
        "\n",
        "#     # Create an instance of the EvaluationPerformance class\n",
        "#     evaluator = EvaluationPerformance(radix_trie, weight_settings)\n",
        "\n",
        "#     # Test data for evaluation\n",
        "#     test_data = [\n",
        "#         (\"PgeprjcessedgData\", \"ProcessedData\"),\n",
        "#         (\"dtae\", \"date\"),\n",
        "#         (\"contknt\", \"contact\"),\n",
        "#         # ... add more test cases\n",
        "#     ]\n",
        "\n",
        "#     # Evaluate and print results\n",
        "#     evaluation_results = evaluator.evaluate(test_data)\n",
        "#     for query, correct_term, overall_score in evaluation_results:\n",
        "#         print(f\"Query: {query}\")\n",
        "#         print(f\"Correct Term: {correct_term}\")\n",
        "#         print(f\"Overall Score: {overall_score}\")\n",
        "#         print(\"---\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # # Create an instance of the MisspelledWordGenerator class\n",
        "    # generator = MisspelledWordGenerator()\n",
        "\n",
        "    # # Collect preprocessed words for batch writes\n",
        "    # radix_batch_data = []\n",
        "    # hashmap_batch_data = []\n",
        "\n",
        "\n",
        "    # Read all values from the original_data worksheet\n",
        "    # data = sheet.get_all_values()\n",
        "\n",
        "  # # Generate and save misspelled words to the test_data worksheet\n",
        "  #   for word in train_words:\n",
        "  #     misspelled_word = generator.generate(word)\n",
        "  #     test_data_sheet.append_row([misspelled_word])\n",
        "\n",
        "  #   print(\"Misspelled words generated and saved to test_data worksheet.\")\n",
        "\n",
        "\n",
        "    # # Preprocess\n",
        "    # for row in data:\n",
        "    #     for word in row:\n",
        "    #         preprocessed_words = preprocess(word)\n",
        "    #         for preprocessed_word in preprocessed_words:\n",
        "    #             if preprocessed_word:  # Skip empty words after preprocessing\n",
        "    #                 radix_trie.insert(preprocessed_word)\n",
        "    #                 hashmap_trie.insert(preprocessed_word)\n",
        "\n",
        "    #                 hashmap_trie.insert(preprocessed_word)\n",
        "    #                 hashmap_batch_data.append([preprocessed_word])\n",
        "\n",
        "    #                 # Use batch writes to save preprocessed words in one API call for each trie structure\n",
        "    #                 # train_data_sheet.append_rows(radix_batch_data)\n",
        "    #                 # train_data_sheet.append_rows(hashmap_batch_data)\n",
        "\n",
        "    # # User input for autocorrection\n",
        "    # while True:\n",
        "    #     user_input = input(\"Enter a word (or type 'exit' to quit): \")\n",
        "    #     if user_input.lower() == 'exit':\n",
        "    #         break\n",
        "\n",
        "    #     # Fetch correct words from the train_data sheet\n",
        "    #     correct_words = train_data_sheet.col_values(1)  # Assuming correct words are in the first column\n",
        "\n",
        "    #     # Autocorrect the user input using Levenshtein distance\n",
        "    #     closest_match = None\n",
        "    #     closest_distance = float('inf')\n",
        "    #     for correct_word in correct_words:\n",
        "    #         distance = Levenshtein.distance(user_input, correct_word)\n",
        "    #         if distance < closest_distance:\n",
        "    #             closest_distance = distance\n",
        "    #             closest_match = correct_word\n",
        "\n",
        "    #     # Display the closest match and append it to the train_data sheet\n",
        "    #     print(f\"Closest Match: {closest_match}\")\n",
        "    #     if closest_match != user_input:\n",
        "    #         train_data_sheet.append_row([f\"User Input: {user_input}, Closest Match: {closest_match}\"])"
      ],
      "metadata": {
        "id": "N4Tx5ff6WldB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}